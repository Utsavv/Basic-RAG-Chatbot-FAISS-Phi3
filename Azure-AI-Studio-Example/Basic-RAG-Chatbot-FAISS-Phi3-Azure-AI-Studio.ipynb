{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic RAG Chatbot with FAISS and Microsoft Phi3\n",
    "\n",
    "This notebook demonstrates how to build a basic Retrieval-Augmented Generation (RAG) chatbot using FAISS for document retrieval and Microsoft Phi3 for generating responses. This guide will help you set up the environment, index documents, and create a web-based chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up Conda Environment and Install Dependencies\n",
    "\n",
    "Create a new Conda environment and install the required libraries by running the following commands in your terminal. FAISS setup instructions are available at https://github.com/facebookresearch/faiss/blob/main/INSTALL.md Please pay attention to GPU setup. If you have CPU only then execute CPU only command. Perfroamnce of CPU only setup will not be as good as GPU setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (21785987.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    $envName = \"BasicRAGChatBot\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define the environment name\n",
    "$envName = \"BasicRAGChatBot\"\n",
    "\n",
    "# Check if the environment already exists\n",
    "$envExists = conda info --envs | Select-String $envName\n",
    "\n",
    "if ($envExists) {\n",
    "    Write-Output \"Environment '$envName' already exists.\"\n",
    "} else {\n",
    "    Write-Output \"Creating environment '$envName'...\"\n",
    "    conda create -n $envName python=3.9 -y\n",
    "}\n",
    "\n",
    "# Activate the environment\n",
    "conda activate $envName\n",
    "\n",
    "\n",
    "# CPU-only version\n",
    "conda install -c pytorch faiss-cpu=1.9.0\n",
    "\n",
    "# GPU(+CPU) version\n",
    "#$ conda install -c pytorch -c nvidia faiss-gpu=1.9.0\n",
    "\n",
    "# GPU(+CPU) version with NVIDIA RAFT\n",
    "\n",
    "#Install dependencies required for Tunning Phi 3.5\n",
    "pip install onnxruntime fastapi uvicorn\n",
    "pip install -q faiss-cpu flask transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded microsoft/Phi-3-mini-128k-instruct-onnx as I am having CPU only laptop. if you have followed instructions, Let us quickly test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:5272/v1/\", \n",
    "    api_key=\"x\" # required for the API but not used\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is the golden ratio?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"Phi-3-mini-4k-cuda-int4-onnx\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to setup Phi3. Easiest way I found to use Phi3 is to use AI Toolkit for Visual Studio Code Detailed instructions at the time of writing this notebook are available at https://github.com/microsoft/vscode-ai-toolkit Again, you need to pay attention whether you are going for CPU only version or GPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import necessary libraries for the notebook. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create FAISS Index\n",
    "\n",
    "Use FAISS to index a document file. We'll use `documentation.txt` (assuming it's in the same directory) as a sample document. This text is created on the basis of public SAP rule engine documentation available at https://help.sap.com/docs/SAP_COMMERCE/9d346683b0084da2938be8a285c0c27a/ba076fa614e549309578fba7159fe628.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a function which will read documentation.txt and create index on the basis of text available in documentation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def create_faiss_index(file_path):\n",
    "    # Step 1: Read the content from the text file    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    # Step 2: Initialize the embedding model\n",
    "    class EmbeddingModel:\n",
    "        _model_instance = None\n",
    "\n",
    "        @classmethod\n",
    "        def get_model(cls):\n",
    "            if cls._model_instance is None:\n",
    "                cls._model_instance = SentenceTransformer('all-MiniLM-L6-v2')  # Load the model only once\n",
    "                print(\"Model loaded successfully.\")\n",
    "            return cls._model_instance\n",
    "\n",
    "    # Step 3: Generate embeddings for each text\n",
    "    model = EmbeddingModel.get_model()\n",
    "    embeddings = model.encode(texts)\n",
    "\n",
    "    # Step 4: Convert embeddings to numpy array\n",
    "    embeddings_np = np.array(embeddings).astype('float32')  # FAISS requires float32 format\n",
    "\n",
    "    # Step 5: Set up FAISS index\n",
    "    embedding_dim = embeddings_np.shape[1]  # Dimensionality of embeddings\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # L2 distance for similarity search\n",
    "\n",
    "    # Step 6: Add embeddings to the FAISS index\n",
    "    index.add(embeddings_np)\n",
    "    print(f\"Indexed {index.ntotal} documents into FAISS\")\n",
    "\n",
    "    # Step 7: Save the index to disk (optional, for persistence)\n",
    "    faiss.write_index(index, 'faiss_index.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will call the above function and create vector DB index using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FAISS Index\n",
    "faiss_index = faiss_impl.create_faiss_index(\"documentation.txt\")\n",
    "print(f\"FAISS index created successfully: {faiss_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Querying the FAISS Index\n",
    "\n",
    "To test the retrieval function, use `search_faiss_index` to query the index with a sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query to test the FAISS index\n",
    "query = \"Explain the rewards feature in detail\"\n",
    "search_results = faiss_impl.search_faiss_index(query)\n",
    "print(\"Top 3 search results:\", search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prompt Engineering\n",
    "\n",
    "Define the prompt for the chatbot to generate responses as a 'Rewards Feature Expert'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(response):\n",
    "    result = faiss_impl.search_faiss_index(response)\n",
    "    prompt = (\n",
    "        f\"\"\"\n",
    "        [Context: Rewards Feature]\n",
    "        [Role: Reward feature expert]\n",
    "        Assume you are a helpful assistant for Reward feature.\n",
    "        Please do not hallucinate.\n",
    "        Say 'I don't know' if you don't know the answer.\n",
    "        Considering all given above, analyze reward feature and summarize following : {result}\n",
    "        \"\"\"\n",
    "    )\n",
    "    return result + \"\\n\\n\" + generate_response('tunedModels/rewardchatbot', prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Responses with Language Model\n",
    "\n",
    "Define the `generate_response` function to connect with Microsoft Phi3 model (or any offline LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def generate_content(self, prompt):\n",
    "        return type('GeneratedContent', (object,), {'text': f\"Generated response for: {prompt}\"})\n",
    "\n",
    "def generate_response(model_name, prompt):\n",
    "    model = GenerativeModel(model_name)\n",
    "    model_output = model.generate_content(prompt)\n",
    "    return model_output.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generating a response\n",
    "test_response = process_question(\"What is the purpose of the rewards feature?\")\n",
    "print(\"Generated Response:\", test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create the Flask API\n",
    "\n",
    "Set up a Flask API to serve as an endpoint for the chatbot, allowing external applications to query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    if not request.is_json:\n",
    "        return jsonify({\"error\": \"Request must be JSON formatted\"}), 400\n",
    "    data = request.get_json()\n",
    "    response = data.get('query')\n",
    "    answer = process_question(response)\n",
    "    return jsonify({\"response\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the Flask app, save this notebook and run the following command in a terminal:\n",
    "```bash\n",
    "flask run --host=0.0.0.0 --port=5000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You now have a fully functional offline RAG chatbot using FAISS and Microsoft Phi3! This setup demonstrates how to retrieve relevant information from documents and respond with a locally hosted language model, ideal for customer support or FAQ applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
