{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Building a Basic RAG Chatbot with FAISS and Microsoft Phi3\n",
       "\n",
       "This notebook demonstrates how to build a basic Retrieval-Augmented Generation (RAG) chatbot using FAISS for document retrieval and Microsoft Phi3 (or a compatible offline LLM) for generating responses. This guide will help you set up the environment, index documents, and create a web-based chatbot."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 1: Set up Conda Environment and Install Dependencies\n",
       "\n",
       "Create a new Conda environment and install the required libraries by running the following commands in your terminal:\n",
       "```bash\n",
       "conda create -n BasicRAGChatBot python=3.9\n",
       "conda activate BasicRAGChatBot\n",
       "pip install faiss-cpu flask transformers torch\n",
       "```\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 2: Import Libraries\n",
       "\n",
       "Import necessary libraries for the notebook. Ensure you have `VectorDBThroughFaissImplementation.py` in the same directory to access FAISS indexing functions."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import faiss\n",
       "from flask import Flask, request, jsonify\n",
       "import numpy as np\n",
       "import sys\n",
       "\n",
       "# Load the FAISS indexing code from the provided file\n",
       "sys.path.insert(0, '/mnt/data/')  \n",
       "import VectorDBThroughFaissImplementation as faiss_impl"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 3: Create FAISS Index\n",
       "\n",
       "Use FAISS to index a document file. We'll use `documentation.txt` (assuming it's in the same directory) as a sample document."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Creating the FAISS Index\n",
       "faiss_index = faiss_impl.create_faiss_index(\"documentation.txt\")\n",
       "print(f\"FAISS index created successfully: {faiss_index}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 4: Test Querying the FAISS Index\n",
       "\n",
       "To test the retrieval function, use `search_faiss_index` to query the index with a sample question."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Sample query to test the FAISS index\n",
       "query = \"Explain the rewards feature in detail\"\n",
       "search_results = faiss_impl.search_faiss_index(query)\n",
       "print(\"Top 3 search results:\", search_results)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 5: Prompt Engineering\n",
       "\n",
       "Define the prompt for the chatbot to generate responses as a 'Rewards Feature Expert'."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def process_question(response):\n",
       "    result = faiss_impl.search_faiss_index(response)\n",
       "    prompt = (\n",
       "        f\"\"\"\n",
       "        [Context: Rewards Feature]\n",
       "        [Role: Reward feature expert]\n",
       "        Assume you are a helpful assistant for Reward feature.\n",
       "        Please do not hallucinate.\n",
       "        Say 'I don't know' if you don't know the answer.\n",
       "        Considering all given above, analyze reward feature and summarize following : {result}\n",
       "        \"\"\"\n",
       "    )\n",
       "    return result + \"\\n\\n\" + generate_response('tunedModels/rewardchatbot', prompt=prompt)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 6: Generate Responses with Language Model\n",
       "\n",
       "Define the `generate_response` function to connect with Microsoft Phi3 model (or any offline LLM)."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "class GenerativeModel:\n",
       "    def __init__(self, model_name):\n",
       "        self.model_name = model_name\n",
       "    \n",
       "    def generate_content(self, prompt):\n",
       "        return type('GeneratedContent', (object,), {'text': f\"Generated response for: {prompt}\"})\n",
       "\n",
       "def generate_response(model_name, prompt):\n",
       "    model = GenerativeModel(model_name)\n",
       "    model_output = model.generate_content(prompt)\n",
       "    return model_output.text"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Test generating a response\n",
       "test_response = process_question(\"What is the purpose of the rewards feature?\")\n",
       "print(\"Generated Response:\", test_response)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Step 7: Create the Flask API\n",
       "\n",
       "Set up a Flask API to serve as an endpoint for the chatbot, allowing external applications to query it."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "app = Flask(__name__)\n",
       "\n",
       "@app.route('/chat', methods=['POST'])\n",
       "def chat():\n",
       "    if not request.is_json:\n",
       "        return jsonify({\"error\": \"Request must be JSON formatted\"}), 400\n",
       "    data = request.get_json()\n",
       "    response = data.get('query')\n",
       "    answer = process_question(response)\n",
       "    return jsonify({\"response\": answer})"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "To start the Flask app, save this notebook and run the following command in a terminal:\n",
       "```bash\n",
       "flask run --host=0.0.0.0 --port=5000\n",
       "```\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "You now have a fully functional offline RAG chatbot using FAISS and Microsoft Phi3! This setup demonstrates how to retrieve relevant information from documents and respond with a locally hosted language model, ideal for customer support or FAQ applications."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
   